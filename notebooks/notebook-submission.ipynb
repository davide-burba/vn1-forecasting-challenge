{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e0fccdd-f0a9-4d82-8f35-d9be0b0f8723",
   "metadata": {},
   "source": [
    "#Â Create Submission for VN1 forecasting competition\n",
    "\n",
    "_Created by: [Davide Burba](https://www.linkedin.com/in/davide-burba/)_\n",
    "\n",
    "In this notebook I included the code used to produce the submission for the \n",
    "[vn1 forecasting competition](https://www.datasource.ai/en/users/davide-burba/competitions/phase-2-vn1-forecasting-accuracy-challenge/datathon_detail/rules).\n",
    "\n",
    "I built this notebook only for the sake of submitting the code, but in reality I used scripts and multiple python modules to produce the submission. I copied the code here, but for this reason looking at the original codebase might be more user friendly.\n",
    "\n",
    "Anyway, if my submission will get a decent score (hope so!), I will open source the code at: [github.com/davide-burba/vn1-forecasting-challenge](https://github.com/davide-burba/vn1-forecasting-challenge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c808cb0b-8379-41aa-91ea-25ea6077e98f",
   "metadata": {},
   "source": [
    "## Python modules \n",
    "\n",
    "Original relative imports are removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4146d1-df13-48b9-b6ea-fd56f446af9c",
   "metadata": {},
   "source": [
    "**vn1/config.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2d4794-85b7-49ad-aace-5f7c3343fdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Literal\n",
    "\n",
    "import mlflow\n",
    "import yaml\n",
    "from pydantic import BaseModel, ConfigDict\n",
    "\n",
    "\n",
    "class FeatureEng(BaseModel):\n",
    "    kind: str\n",
    "    params: dict[str, Any]\n",
    "\n",
    "\n",
    "class DataFeatureEng(BaseModel):\n",
    "    source: Literal[\"sales\", \"price\"]\n",
    "    groupby: list[str] | None = None\n",
    "    group_stat: Literal[\"sum\", \"mean\", \"std\", \"min\", \"max\"] = \"sum\"\n",
    "    feature_eng_list: list[FeatureEng]\n",
    "\n",
    "    model_config = ConfigDict(extra=\"forbid\")\n",
    "\n",
    "\n",
    "class StaticFeature(BaseModel):\n",
    "    name: str\n",
    "    categorical: bool\n",
    "\n",
    "    model_config = ConfigDict(extra=\"forbid\")\n",
    "\n",
    "\n",
    "class PreprocessingConfig(BaseModel):\n",
    "    data_feature_eng_list: list[DataFeatureEng]\n",
    "    static_feature_list: list[StaticFeature]\n",
    "    date_features: list[Literal[\"year\", \"month\", \"day\"]]\n",
    "    normalize_price: bool = True\n",
    "    normalize_sales: bool = True\n",
    "\n",
    "    model_config = ConfigDict(extra=\"forbid\")\n",
    "\n",
    "\n",
    "class Config(BaseModel):\n",
    "    preprocessing_config: PreprocessingConfig\n",
    "    engine_params: dict[str, Any]\n",
    "\n",
    "    multi_horizon: bool = True\n",
    "    include_horizon_feature: bool = True\n",
    "    include_horizon_year: bool = False\n",
    "    include_horizon_month: bool = False\n",
    "    include_horizon_day: bool = False\n",
    "\n",
    "    magic_multiplier: float = 1.0\n",
    "\n",
    "    model_config = ConfigDict(extra=\"forbid\")\n",
    "\n",
    "\n",
    "def load_config(path):\n",
    "    with open(path, \"r\") as file:\n",
    "        config_data = yaml.safe_load(file)\n",
    "    return Config(**config_data)\n",
    "\n",
    "\n",
    "def track_config_with_mlflow(config):\n",
    "    # track the most important attributes of the config with mlfow\n",
    "    mlflow.log_params(config.engine_params)\n",
    "    mlflow.log_params(config.preprocessing_config.model_dump())\n",
    "    mlflow.log_param(\"multi_horizon\", config.multi_horizon)\n",
    "    mlflow.log_param(\"include_horizon_feature\", config.include_horizon_feature)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d94d96c-83c4-4b79-8975-9878fb8eee62",
   "metadata": {},
   "source": [
    "**vn1/data_loading.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be16719b-5c71-4906-9858-11f4b112ac33",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_data(phase, path):\n",
    "    assert phase in {1, 2}\n",
    "    path = Path(path)\n",
    "    sales_0 = pd.read_csv(path / \"Phase 0 - Sales.csv\")\n",
    "    price_0 = pd.read_csv(path / \"Phase 0 - Price.csv\")\n",
    "\n",
    "    if phase == 1:\n",
    "        return sales_0, price_0\n",
    "\n",
    "    sales_1 = pd.read_csv(path / \"Phase 1 - Sales.csv\")\n",
    "    price_1 = pd.read_csv(path / \"Phase 1 - Price.csv\")\n",
    "\n",
    "    sales = pd.concat([sales_0, sales_1[sales_1.columns[3:]]], axis=1)\n",
    "    price = pd.concat([price_0, price_1[price_1.columns[3:]]], axis=1)\n",
    "\n",
    "    return sales, price\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d194392-8e99-47a0-a269-7dc25e1d9637",
   "metadata": {},
   "source": [
    "**vn1/score.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28089b81-15c2-47d1-959f-7f96dca96458",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_competition_score(submission, objective):\n",
    "    abs_err = np.nansum(abs(submission - objective))\n",
    "    err = np.nansum((submission - objective))\n",
    "    score = abs_err + abs(err)\n",
    "    score /= objective.sum().sum()\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebe7b1f-93b3-4a92-bb81-2c2a541e271d",
   "metadata": {},
   "source": [
    "**vn1/preprocessing.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482337cf-9e32-4cb8-a7a5-1e85de8967e1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ID_COLS = (\"Client\", \"Warehouse\", \"Product\")\n",
    "HORIZONS = (1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13)\n",
    "\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self, static_feature_list, data_feature_eng_list, date_features):\n",
    "        for col in static_feature_list:\n",
    "            assert col.name in ID_COLS\n",
    "        self.static_feature_map = {feat.name: feat for feat in static_feature_list}\n",
    "        self.data_feature_eng_list = data_feature_eng_list\n",
    "        self.date_features = date_features\n",
    "\n",
    "    def prepare_data(self, sales, price):\n",
    "        targets = self.prepare_targets(sales)\n",
    "        features = self.prepare_features(sales, price)\n",
    "\n",
    "        return targets, features\n",
    "\n",
    "    def prepare_targets(self, sales):\n",
    "        sales_ts = sales.set_index(list(ID_COLS))\n",
    "        all_targets = {}\n",
    "        for horizon in HORIZONS:\n",
    "            df = (\n",
    "                build_lagged_df(sales_ts, -horizon)\n",
    "                .melt(ignore_index=False)\n",
    "                .reset_index()\n",
    "                .rename(columns={\"variable\": \"time\"})\n",
    "            )\n",
    "            df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "            all_targets[horizon] = df\n",
    "\n",
    "        return all_targets\n",
    "\n",
    "    def prepare_features(self, sales, price):\n",
    "        all_features = []\n",
    "        for data_feature_eng in self.data_feature_eng_list:\n",
    "            # set time-series data on which to apply feature engineering\n",
    "            data_name = self._build_data_name(\n",
    "                data_feature_eng.source, data_feature_eng.groupby\n",
    "            )\n",
    "            match data_feature_eng.source:\n",
    "                case \"sales\":\n",
    "                    df_input = sales\n",
    "                case \"price\":\n",
    "                    df_input = price\n",
    "\n",
    "            df_ts = self._group_timeseries(\n",
    "                df_input,\n",
    "                data_feature_eng.groupby,\n",
    "                data_feature_eng.group_stat,\n",
    "            )\n",
    "\n",
    "            # engineer features\n",
    "            for feat_eng in data_feature_eng.feature_eng_list:\n",
    "                feat = self._apply_feat_eng(\n",
    "                    feat_eng.kind, feat_eng.params, df_ts, data_name\n",
    "                )\n",
    "                all_features.append(feat)\n",
    "\n",
    "        features = self._merge_all_features(all_features, sales)\n",
    "\n",
    "        # Static features\n",
    "        for col in ID_COLS:\n",
    "            # drop if not listed\n",
    "            if col not in self.static_feature_map:\n",
    "                features = features.drop(columns=col)\n",
    "            # convert to category if specified\n",
    "            elif self.static_feature_map[col].categorical:\n",
    "                features[col] = features[col].astype(\"category\")\n",
    "\n",
    "        # Date features\n",
    "        for date_feat in self.date_features:\n",
    "            match date_feat:\n",
    "                case \"year\":\n",
    "                    features[\"year\"] = features[\"time\"].dt.year\n",
    "                case \"month\":\n",
    "                    features[\"month\"] = features[\"time\"].dt.month\n",
    "                case \"day\":\n",
    "                    features[\"day\"] = features[\"time\"].dt.day\n",
    "                case _:\n",
    "                    raise ValueError(f\"Unknown date feature: {date_feat}\")\n",
    "\n",
    "        return features.drop(columns=[\"time\"])\n",
    "\n",
    "    def _apply_feat_eng(self, kind, params, df_ts, data_name):\n",
    "        match kind:\n",
    "            case \"lag\":\n",
    "                lag = params[\"lag\"]\n",
    "                feature_name = f\"{data_name}_lag-{lag}\"\n",
    "                feat = build_lagged_df(df_ts, lag)\n",
    "            case \"rolling\":\n",
    "                window = params[\"window\"]\n",
    "                statistic = params[\"statistic\"]\n",
    "                feature_name = f\"{data_name}_rolling-{statistic}_w{window}\"\n",
    "                feat = build_rolling_statistic_df(df_ts, window, statistic)\n",
    "            case _:\n",
    "                raise ValueError(f\"Unknown feature engineering kind: {kind}\")\n",
    "\n",
    "        return self._format_engineered_feature(feat, feature_name)\n",
    "\n",
    "    def _merge_all_features(self, all_features, sales):\n",
    "        join_df = (\n",
    "            sales.set_index(list(ID_COLS))\n",
    "            .melt(ignore_index=False)\n",
    "            .reset_index()\n",
    "            .rename(columns={\"variable\": \"time\"})[list(ID_COLS) + [\"time\"]]\n",
    "        )\n",
    "        join_df[\"time\"] = pd.to_datetime(join_df[\"time\"])\n",
    "\n",
    "        # merge all with join_df\n",
    "        features_df = join_df.copy()\n",
    "        for feat in all_features:\n",
    "            for_join, join_on = self._prepare_for_join(feat)\n",
    "            features_df = features_df.merge(for_join, on=join_on, how=\"left\")\n",
    "\n",
    "        return features_df\n",
    "\n",
    "    def _group_timeseries(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        groupby: list[str] | None = None,\n",
    "        group_stat: str = \"sum\",\n",
    "    ) -> pd.DataFrame:\n",
    "        ts = df.copy()\n",
    "\n",
    "        index_cols = groupby or list(ID_COLS)\n",
    "\n",
    "        if groupby is not None:\n",
    "            for col in groupby:\n",
    "                assert col in ID_COLS\n",
    "\n",
    "            cols_to_drop = [c for c in ID_COLS if c not in groupby]\n",
    "            ts = ts.drop(columns=cols_to_drop)\n",
    "\n",
    "            # ts = ts.groupby(groupby).sum().reset_index()\n",
    "            ts = ts.groupby(groupby).agg(group_stat).reset_index()\n",
    "\n",
    "        return ts.set_index(index_cols)\n",
    "\n",
    "    def _prepare_for_join(self, df):\n",
    "        join_on = list(df.index.names) + [\"time\"]\n",
    "        df_for_join = df.reset_index()\n",
    "        return df_for_join, join_on\n",
    "\n",
    "    def _build_data_name(self, source, groupby):\n",
    "        return f\"{source}_{'' if groupby is None else '-'.join(groupby)}\"\n",
    "\n",
    "    def _format_engineered_feature(self, feat, feature_name):\n",
    "        feat = feat.melt(ignore_index=False)\n",
    "        feat = feat.rename(columns={\"value\": feature_name, \"variable\": \"time\"})\n",
    "        feat[\"time\"] = pd.to_datetime(feat[\"time\"])\n",
    "        return feat\n",
    "\n",
    "\n",
    "def build_lagged_df(df, lag=0):\n",
    "    return df.shift(lag, axis=1)\n",
    "\n",
    "\n",
    "def build_rolling_statistic_df(df, window=4, statistic=\"mean\"):\n",
    "    return df.T.rolling(window).agg(statistic).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf93d928-f4fb-406b-a7f6-ee4751816cc1",
   "metadata": {},
   "source": [
    "**submission.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67c29c8-5c76-4ee5-b8f5-bdfc477b7778",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "# from vn1.preprocessing import ID_COLS\n",
    "\n",
    "\n",
    "def validate_submission(submission, example_sumbission_path):\n",
    "    example_submission = pd.read_csv(example_sumbission_path)\n",
    "\n",
    "    assert (submission.columns == example_submission.columns).all()\n",
    "\n",
    "    base_cols = list(ID_COLS)\n",
    "    pd.testing.assert_frame_equal(submission[base_cols], example_submission[base_cols])\n",
    "\n",
    "\n",
    "def track_submission_with_mlflow(submission):\n",
    "    run_id = mlflow.active_run().info.run_id\n",
    "    path = f\"/tmp/submission_{run_id}.csv\"\n",
    "    submission.to_csv(path, index=False)\n",
    "    mlflow.log_artifact(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc5e566-2415-40cd-8635-25a3cc74b0e4",
   "metadata": {},
   "source": [
    "**vn1/forecaster.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231424a9-678e-47ad-a228-95dd28bc76b7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# from vn1.preprocessing import HORIZONS, ID_COLS, Preprocessor\n",
    "# from vn1.score import compute_competition_score\n",
    "\n",
    "\n",
    "class Forecaster:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "        self._model = None\n",
    "        self._price_scaler = None\n",
    "        self._sales_norm_data = None\n",
    "\n",
    "    def build_future_predictions(self, sales, price):\n",
    "        print(\"preprocess\")\n",
    "        sales, price = sales.copy(), price.copy()\n",
    "        sales, price = self._normalize_sales_and_price(sales, price)\n",
    "        targets, features = self._prepare_targets_and_features(sales, price)\n",
    "\n",
    "        if self.config.multi_horizon:\n",
    "            future_predictions = self._predict_multi_horizon(targets, features)\n",
    "        else:\n",
    "            future_predictions = self._predict_by_horizon(targets, features)\n",
    "\n",
    "        return self._inverse_normalize(future_predictions)\n",
    "\n",
    "    def cross_validate(self, sales, price, n_folds=4, test_size=0.05):\n",
    "        print(\"preprocess\")\n",
    "        sales, price = sales.copy(), price.copy()\n",
    "        # slight leakage by using min/max of all data\n",
    "        sales, price = self._normalize_sales_and_price(sales, price)\n",
    "        targets, features = self._prepare_targets_and_features(sales, price)\n",
    "\n",
    "        if self.config.multi_horizon:\n",
    "            self._cross_validate_multi_horizon(features, targets, n_folds, test_size)\n",
    "        else:\n",
    "            self._cross_validate_by_horizon(features, targets, n_folds, test_size)\n",
    "\n",
    "    def _predict_multi_horizon(\n",
    "        self,\n",
    "        targets: dict[int, pd.DataFrame],\n",
    "        features: pd.DataFrame,\n",
    "    ):\n",
    "        print(\"Training multi horizon\")\n",
    "        # prepare data in x,y format\n",
    "        multi_h_target, multi_h_features = self._prepare_data_multi_horizon(\n",
    "            targets, features\n",
    "        )\n",
    "\n",
    "        # train model\n",
    "        # Important: drop the target NA, otherwise treated as 0!\n",
    "        print(\"Training model\")\n",
    "        mask_na = multi_h_target.value.isna()\n",
    "        multi_h_target_train = multi_h_target[~mask_na].reset_index(drop=True)\n",
    "        multi_h_features_train = multi_h_features[~mask_na].reset_index(drop=True)\n",
    "        self._model = LGBMRegressor(**self.config.engine_params)\n",
    "        self._model.fit(multi_h_features_train, multi_h_target_train.value)\n",
    "\n",
    "        # inference\n",
    "        print(\"Inference\")\n",
    "        last_timestep = multi_h_target.time.max()\n",
    "        mask = multi_h_target.time == last_timestep\n",
    "        inference_features = multi_h_features[mask].reset_index(drop=True)\n",
    "        inference_predictions = (\n",
    "            multi_h_target[mask].drop(columns=[\"value\", \"time\"]).reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        inference_predictions[\"y_pred\"] = self._predict_with_magic_multiplier(\n",
    "            inference_features\n",
    "        )\n",
    "\n",
    "        # format output\n",
    "        print(\"Formatting output\")\n",
    "        id_cols = list(ID_COLS)\n",
    "        future_predictions = targets[1][targets[1].time == last_timestep][\n",
    "            id_cols\n",
    "        ].reset_index(drop=True)\n",
    "        for horizon in HORIZONS:\n",
    "            pred_horizon = inference_predictions[\n",
    "                inference_predictions.horizon == horizon\n",
    "            ].drop(columns=[\"horizon\"])\n",
    "\n",
    "            future_predictions = pd.merge(\n",
    "                future_predictions,\n",
    "                pred_horizon,\n",
    "                on=id_cols,\n",
    "                how=\"left\",\n",
    "            ).rename(columns={\"y_pred\": f\"pred_{horizon}\"})\n",
    "\n",
    "        return self._format_inference(future_predictions, last_timestep)\n",
    "\n",
    "    def _predict_with_magic_multiplier(self, features: pd.DataFrame):\n",
    "        return self._model.predict(features) * self.config.magic_multiplier\n",
    "\n",
    "    def _prepare_data_multi_horizon(self, targets, features):\n",
    "        # prepare data in x,y format\n",
    "        multi_h_features_list = []\n",
    "        multi_h_target_list = []\n",
    "        for horizon in HORIZONS:\n",
    "            # build target\n",
    "            targets_h = targets[horizon].copy()\n",
    "            targets_h[\"horizon\"] = horizon\n",
    "            multi_h_target_list.append(targets_h)\n",
    "\n",
    "            # build features\n",
    "            features_h = features.copy()\n",
    "\n",
    "            if self.config.include_horizon_feature:\n",
    "                features_h[\"horizon\"] = horizon\n",
    "\n",
    "            horizon_date = targets_h.time + pd.offsets.Week(horizon)\n",
    "            if self.config.include_horizon_year:\n",
    "                features_h[\"horizon_year\"] = horizon_date.dt.year\n",
    "            if self.config.include_horizon_month:\n",
    "                features_h[\"horizon_month\"] = horizon_date.dt.month\n",
    "            if self.config.include_horizon_day:\n",
    "                features_h[\"horizon_day\"] = horizon_date.dt.day\n",
    "\n",
    "            multi_h_features_list.append(features_h)\n",
    "\n",
    "        multi_h_target = pd.concat(multi_h_target_list).reset_index(drop=True)\n",
    "        multi_h_features = pd.concat(multi_h_features_list).reset_index(drop=True)\n",
    "\n",
    "        return multi_h_target, multi_h_features\n",
    "\n",
    "    def _predict_by_horizon(\n",
    "        self,\n",
    "        targets: dict[int, pd.DataFrame],\n",
    "        features: pd.DataFrame,\n",
    "    ):\n",
    "        print(\"Training by horizon\")\n",
    "\n",
    "        last_timestep = targets[1].time.max()\n",
    "        mask = targets[1].time == last_timestep\n",
    "        id_cols = list(ID_COLS)\n",
    "        future_predictions = targets[1][mask][id_cols].reset_index(drop=True)\n",
    "\n",
    "        for horizon in HORIZONS:\n",
    "            self._model = LGBMRegressor(**self.config.engine_params)\n",
    "            self._model.fit(features, targets[horizon].value)\n",
    "\n",
    "            mask = targets[horizon].time == last_timestep\n",
    "            inference_features = features[mask].reset_index(drop=True)\n",
    "            inference_targets = (\n",
    "                targets[horizon][mask]\n",
    "                .reset_index(drop=True)\n",
    "                .drop(columns=[\"value\", \"time\"])\n",
    "            )\n",
    "            inference_targets[\"y_pred\"] = self._predict_with_magic_multiplier(\n",
    "                inference_features\n",
    "            )\n",
    "            future_predictions = pd.merge(\n",
    "                future_predictions,\n",
    "                inference_targets,\n",
    "                on=id_cols,\n",
    "                how=\"left\",\n",
    "            ).rename(columns={\"y_pred\": f\"pred_{horizon}\"})\n",
    "\n",
    "        return self._format_inference(future_predictions, last_timestep)\n",
    "\n",
    "    def _format_inference(self, future_predictions, last_timestep):\n",
    "        return future_predictions.rename(\n",
    "            columns={\n",
    "                f\"pred_{h}\": (\n",
    "                    pd.Timestamp(last_timestep) + pd.offsets.Week(h)\n",
    "                ).strftime(\"%Y-%m-%d\")\n",
    "                for h in HORIZONS\n",
    "            }\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "    def _cross_validate_multi_horizon(self, features, targets, n_folds, test_size):\n",
    "        print(\"CV multi horizon\")\n",
    "        # prepare data in x,y format\n",
    "        multi_h_target, multi_h_features = self._prepare_data_multi_horizon(\n",
    "            targets, features\n",
    "        )\n",
    "        # Important: drop the target NA, otherwise treated as 0!\n",
    "        mask_na = multi_h_target.value.isna()\n",
    "        multi_h_target = multi_h_target[~mask_na].reset_index(drop=True)\n",
    "        multi_h_features = multi_h_features[~mask_na].reset_index(drop=True)\n",
    "\n",
    "        # Time-based cross-validation\n",
    "        end_quantile = 1\n",
    "        delta_quantile = test_size\n",
    "        scores = []\n",
    "        for fold in range(n_folds):\n",
    "            # Split data by time.\n",
    "            # Not exactly equivalent as by-horizon split, the most recent fold\n",
    "            # will have more short and less far horizons.\n",
    "            # Probably negligible.\n",
    "            end_timestep = multi_h_target.time.quantile(end_quantile)\n",
    "            start_timestep = multi_h_target.time.quantile(end_quantile - delta_quantile)\n",
    "            print(f\"Fold {fold} start: {start_timestep}, end: {end_timestep}\")\n",
    "\n",
    "            mask_train = multi_h_target.time <= start_timestep\n",
    "            mask_test = (multi_h_target.time > start_timestep) & (\n",
    "                multi_h_target.time <= end_timestep\n",
    "            )\n",
    "            x_train = multi_h_features[mask_train]\n",
    "            y_train = multi_h_target[mask_train]\n",
    "            x_test = multi_h_features[mask_test]\n",
    "            y_test = multi_h_target[mask_test].reset_index(drop=True)\n",
    "\n",
    "            # Train model.\n",
    "            self._model = LGBMRegressor(**self.config.engine_params)\n",
    "            self._model.fit(x_train, y_train.value)\n",
    "            y_pred = self._predict_with_magic_multiplier(x_test)\n",
    "\n",
    "            # Inverse normalization by time-series.\n",
    "            y_pred, y_test = self._inverse_normalize_cv(y_pred, y_test)\n",
    "\n",
    "            # Evaluate.\n",
    "            score = compute_competition_score(y_pred, y_test)\n",
    "            print(f\"Fold {fold} score: {score}\")\n",
    "            mlflow.log_metric(f\"score_fold_{fold}\", score)\n",
    "            log_cv_predictions(y_pred, y_test, fold)\n",
    "            end_quantile -= delta_quantile\n",
    "            scores.append(score)\n",
    "\n",
    "        avg_score = sum(scores) / len(scores)\n",
    "        print(f\"Avg score: {avg_score}\")\n",
    "        mlflow.log_metric(\"score_avg\", avg_score)\n",
    "\n",
    "    def _cross_validate_by_horizon(self, features, targets, n_folds, test_size):\n",
    "        end_quantile = 1\n",
    "        delta_quantile = test_size\n",
    "        features = features.reset_index(drop=True)\n",
    "        scores = []\n",
    "        for fold in range(n_folds):\n",
    "            fold_scores = []\n",
    "            for horizon in HORIZONS:\n",
    "                targets_h = targets[horizon].reset_index(drop=True)\n",
    "\n",
    "                # split train/test by time\n",
    "                timesteps_no_na = targets_h.dropna().time\n",
    "                end_timestep = timesteps_no_na.quantile(end_quantile)\n",
    "                start_timestep = timesteps_no_na.quantile(end_quantile - delta_quantile)\n",
    "\n",
    "                mask_train = targets_h.time <= start_timestep\n",
    "                mask_test = (targets_h.time > start_timestep) & (\n",
    "                    targets_h.time <= end_timestep\n",
    "                )\n",
    "                x_train = features[mask_train]\n",
    "                y_train = targets_h[mask_train]\n",
    "                x_test = features[mask_test]\n",
    "                y_test = targets_h[mask_test].reset_index(drop=True)\n",
    "\n",
    "                # Train model.\n",
    "                self._model = LGBMRegressor(**self.config.engine_params)\n",
    "                self._model.fit(x_train, y_train.value)\n",
    "                y_pred = self._predict_with_magic_multiplier(x_test)\n",
    "\n",
    "                # Inverse normalization by time-series.\n",
    "                y_pred, y_test = self._inverse_normalize_cv(y_pred, y_test)\n",
    "\n",
    "                score = compute_competition_score(y_pred, y_test)\n",
    "                mlflow.log_metric(f\"score_horizon_{horizon}_fold_{fold}\", score)\n",
    "                scores.append(score)\n",
    "                fold_scores.append(score)\n",
    "\n",
    "            avg_fold_score = sum(fold_scores) / len(fold_scores)\n",
    "            mlflow.log_metric(f\"score_fold_{fold}\", avg_fold_score)\n",
    "\n",
    "            end_quantile -= delta_quantile\n",
    "        avg_score = sum(scores) / len(scores)\n",
    "        mlflow.log_metric(\"score_avg\", avg_score)\n",
    "\n",
    "    def _normalize_sales_and_price(self, sales, price):\n",
    "        if self.config.preprocessing_config.normalize_price:\n",
    "            self._price_scaler = MinMaxScaler()\n",
    "            price[price.columns[3:]] = self._price_scaler.fit_transform(\n",
    "                price[price.columns[3:]].T\n",
    "            ).T\n",
    "\n",
    "        if self.config.preprocessing_config.normalize_sales:\n",
    "            # Manual normalization, makes it easier to use for cross-validation.\n",
    "            sales_norm_data = sales[sales.columns[:3]].copy()\n",
    "            sales_norm_data[\"min\"] = sales[sales.columns[3:]].T.min()\n",
    "            sales_norm_data[\"max\"] = sales[sales.columns[3:]].T.max()\n",
    "            sales_norm_data[\"delta\"] = sales_norm_data[\"max\"] - sales_norm_data[\"min\"]\n",
    "\n",
    "            sales[sales.columns[3:]] = (\n",
    "                (sales[sales.columns[3:]].T - sales_norm_data[\"min\"])\n",
    "                / sales_norm_data[\"delta\"]\n",
    "            ).T.values\n",
    "\n",
    "            self._sales_norm_data = sales_norm_data\n",
    "\n",
    "        return sales, price\n",
    "\n",
    "    def _inverse_normalize(self, predictions):\n",
    "        if self.config.preprocessing_config.normalize_sales:\n",
    "            pd.testing.assert_frame_equal(\n",
    "                predictions[list(ID_COLS)], self._sales_norm_data[list(ID_COLS)]\n",
    "            )\n",
    "            predictions[predictions.columns[3:]] = (\n",
    "                (\n",
    "                    predictions[predictions.columns[3:]].T\n",
    "                    * self._sales_norm_data[\"delta\"]\n",
    "                )\n",
    "                + self._sales_norm_data[\"min\"]\n",
    "            ).T\n",
    "        # clip negative values\n",
    "        predictions[predictions < 0] = 0\n",
    "        return predictions\n",
    "\n",
    "    def _inverse_normalize_cv(self, y_pred, test_df):\n",
    "        y_test = test_df[\"value\"]\n",
    "        if self.config.preprocessing_config.normalize_sales:\n",
    "            test_norm_data = pd.merge(\n",
    "                test_df,\n",
    "                self._sales_norm_data,\n",
    "                on=ID_COLS,\n",
    "                how=\"left\",\n",
    "            )\n",
    "            y_pred = y_pred * test_norm_data[\"delta\"] + test_norm_data[\"min\"]\n",
    "            y_test = y_test * test_norm_data[\"delta\"] + test_norm_data[\"min\"]\n",
    "        # clip negative values\n",
    "        y_pred[y_pred < 0] = 0\n",
    "        return y_pred, y_test\n",
    "\n",
    "    def _prepare_targets_and_features(self, sales, price):\n",
    "        preprocessor = Preprocessor(\n",
    "            self.config.preprocessing_config.static_feature_list,\n",
    "            self.config.preprocessing_config.data_feature_eng_list,\n",
    "            self.config.preprocessing_config.date_features,\n",
    "        )\n",
    "        return preprocessor.prepare_data(sales, price)\n",
    "\n",
    "\n",
    "def log_cv_predictions(y_pred, y_test, fold):\n",
    "    run_id = mlflow.active_run().info.run_id\n",
    "    path = f\"/tmp/predictions_{run_id}_fold_{fold}.p\"\n",
    "    pd.to_pickle((y_pred, y_test), path)\n",
    "    mlflow.log_artifact(path)\n",
    "    os.remove(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec02535-2d79-4e03-919c-9e34ea7eed44",
   "metadata": {},
   "source": [
    "## Scripts\n",
    "\n",
    "These scripts have been slightly edited to make them runnable within the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6b3fe4-b952-491a-bc4b-175c8be491e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit this path to point to the folder with the raw input files\n",
    "PATH_RAW_DATA = Path(\"../data/raw/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e71288-4b85-4ea6-8f53-55537526ca96",
   "metadata": {},
   "source": [
    "**scripts/run_phase_2.py**\n",
    "\n",
    "This script was used to cross-validate multiple configurations, in order to choose the best combination of feature selection, feature engineering, and hyper-parameters tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d900064-ba30-47b7-bc1e-b89b4e5b9d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import mlflow\n",
    "from fire import Fire\n",
    "\n",
    "#Â from vn1.config import load_config, track_config_with_mlflow\n",
    "#Â from vn1.data_loading import load_data\n",
    "#Â from vn1.forecaster import Forecaster\n",
    "#Â from vn1.submission import track_submission_with_mlflow\n",
    "\n",
    "\n",
    "def run_phase_2(\n",
    "# def main(\n",
    "    config: Config,\n",
    "    #config_path: str = \"config.yaml\",\n",
    "    skip_cross_validation: bool = False,\n",
    "    dump_submission_locally: bool = False,\n",
    "):\n",
    "    mlflow.set_experiment(\"phase_2\")\n",
    "    with mlflow.start_run():\n",
    "        print(\"load inputs\")\n",
    "        #config = load_config(config_path)\n",
    "        sales, price = load_data(phase=2, path=PATH_RAW_DATA)\n",
    "        # mlflow.log_artifact(config_path)\n",
    "        track_config_with_mlflow(config)\n",
    "\n",
    "        forecaster = Forecaster(config)\n",
    "\n",
    "        if not skip_cross_validation:\n",
    "            print(\"cross validate\")\n",
    "            forecaster.cross_validate(sales, price)\n",
    "\n",
    "        print(\"build predictions\")\n",
    "        submission = forecaster.build_future_predictions(sales, price)\n",
    "\n",
    "        print(\"dump\")\n",
    "        if dump_submission_locally:\n",
    "            run_id = mlflow.active_run().info.run_id\n",
    "            os.makedirs(\"submissions\", exist_ok=True)\n",
    "            submission.to_csv(\n",
    "                f\"submissions/submission_phase_2_{run_id}.csv\", index=False\n",
    "            )\n",
    "        track_submission_with_mlflow(submission)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#    Fire(main)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c224a3a9-f82b-410e-9476-f33b96b3a622",
   "metadata": {},
   "source": [
    "**scripts/run_phase_2_ensemble.py**\n",
    "\n",
    "This script was used to ensemble multiple predictions using the same configuration but varying the random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8559cbc6-9476-46b8-ac64-640f14656a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import mlflow\n",
    "from fire import Fire\n",
    "\n",
    "# from vn1.config import load_config, track_config_with_mlflow\n",
    "# from vn1.data_loading import load_data\n",
    "# from vn1.forecaster import Forecaster\n",
    "# from vn1.preprocessing import ID_COLS\n",
    "# from vn1.submission import track_submission_with_mlflow\n",
    "\n",
    "\n",
    "def run_phase_2_ensemble(\n",
    "# def main(\n",
    "    config: Config,\n",
    "    n_estimators: int = 10,\n",
    "    #config_path: str = \"config.yaml\",\n",
    "    dump_submission_locally: bool = True,\n",
    "):\n",
    "    mlflow.set_experiment(\"phase_2_ensemble\")\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        print(\"load inputs\")\n",
    "        # config = load_config(config_path)\n",
    "        sales, price = load_data(phase=2, path=PATH_RAW_DATA)\n",
    "        # mlflow.log_artifact(config_path)\n",
    "        track_config_with_mlflow(config)\n",
    "\n",
    "        run_id = mlflow.active_run().info.run_id\n",
    "        if dump_submission_locally:\n",
    "            output_folder = Path(f\"submissions/submission-ensemble-{run_id}\")\n",
    "            os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        submissions = []\n",
    "        for seed in range(n_estimators):\n",
    "            config.engine_params[\"seed\"] = seed\n",
    "            forecaster = Forecaster(config)\n",
    "\n",
    "            print(\"build predictions\")\n",
    "            submission = forecaster.build_future_predictions(sales, price)\n",
    "\n",
    "            print(\"dump\")\n",
    "            if dump_submission_locally:\n",
    "                submission.to_csv(\n",
    "                    output_folder / f\"submission_seed_{seed}.csv\",\n",
    "                    index=False,\n",
    "                )\n",
    "            submissions.append(submission)\n",
    "\n",
    "        id_cols = list(ID_COLS)\n",
    "        submission = sum([df.set_index(id_cols) for df in submissions]) / n_estimators\n",
    "        submission = submission.reset_index()\n",
    "\n",
    "        track_submission_with_mlflow(submission)\n",
    "        if dump_submission_locally:\n",
    "            submission.to_csv(\n",
    "                output_folder / \"submission_ensemble.csv\",\n",
    "                index=False,\n",
    "            )\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     Fire(main)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51e4d15-e256-4a67-89ea-c57ad53d3468",
   "metadata": {},
   "source": [
    "## Build submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828c04de-3c11-4738-8617-5df34661d01e",
   "metadata": {},
   "source": [
    "**Final configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a7696c-5361-4820-bdcd-26cac0dcb3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_data = {\n",
    " #Â lightgbm params\n",
    " 'engine_params': {'colsample_bytree': 0.6,\n",
    "  'learning_rate': 0.033932217718953266,\n",
    "  'max_depth': 11,\n",
    "  'min_child_samples': 25,\n",
    "  'n_estimators': 488,\n",
    "  'num_leaves': 220,\n",
    "  'subsample': 0.5,\n",
    "  'verbose': 0},\n",
    " #Â multi horizon strategy\n",
    " 'multi_horizon': True,\n",
    " # multi-horizon features\n",
    " 'include_horizon_feature': True,\n",
    " 'include_horizon_day': True,\n",
    " 'include_horizon_month': True,\n",
    " 'include_horizon_year': True,\n",
    " #Â chase the metric: const to multiply predictions\n",
    " # which minimize the cross-validated scores\n",
    " 'magic_multiplier': 0.9955,\n",
    " # preprocessing\n",
    " 'preprocessing_config': {'normalize_price': False,\n",
    "  'normalize_sales': False,\n",
    "  'date_features': ['year', 'month', 'day'],\n",
    "  'static_feature_list': [{'categorical': True, 'name': 'Client'},\n",
    "   {'categorical': True, 'name': 'Warehouse'},\n",
    "   {'categorical': True, 'name': 'Product'}],\n",
    "  # sales feature engineering\n",
    "  'data_feature_eng_list': [{'feature_eng_list': [{'kind': 'lag',\n",
    "      'params': {'lag': 0}},\n",
    "     {'kind': 'lag', 'params': {'lag': 1}},\n",
    "     {'kind': 'lag', 'params': {'lag': 2}},\n",
    "     {'kind': 'lag', 'params': {'lag': 3}},\n",
    "     {'kind': 'lag', 'params': {'lag': 4}},\n",
    "     {'kind': 'lag', 'params': {'lag': 5}},\n",
    "     {'kind': 'lag', 'params': {'lag': 6}},\n",
    "     {'kind': 'lag', 'params': {'lag': 7}},\n",
    "     {'kind': 'lag', 'params': {'lag': 11}},\n",
    "     {'kind': 'lag', 'params': {'lag': 15}},\n",
    "     {'kind': 'lag', 'params': {'lag': 19}},\n",
    "     {'kind': 'lag', 'params': {'lag': 25}},\n",
    "     {'kind': 'lag', 'params': {'lag': 51}},\n",
    "     {'kind': 'lag', 'params': {'lag': 103}},\n",
    "     {'kind': 'rolling', 'params': {'statistic': 'mean', 'window': 4}},\n",
    "     {'kind': 'rolling', 'params': {'statistic': 'mean', 'window': 8}},\n",
    "     {'kind': 'rolling', 'params': {'statistic': 'mean', 'window': 12}},\n",
    "     {'kind': 'rolling', 'params': {'statistic': 'mean', 'window': 26}},\n",
    "     {'kind': 'rolling', 'params': {'statistic': 'mean', 'window': 52}},\n",
    "     {'kind': 'rolling', 'params': {'statistic': 'mean', 'window': 104}},\n",
    "     {'kind': 'rolling', 'params': {'statistic': 'std', 'window': 4}},\n",
    "     {'kind': 'rolling', 'params': {'statistic': 'std', 'window': 8}},\n",
    "     {'kind': 'rolling', 'params': {'statistic': 'std', 'window': 12}},\n",
    "     {'kind': 'rolling', 'params': {'statistic': 'std', 'window': 26}},\n",
    "     {'kind': 'rolling', 'params': {'statistic': 'std', 'window': 52}},\n",
    "     {'kind': 'rolling', 'params': {'statistic': 'std', 'window': 104}},\n",
    "     {'kind': 'rolling', 'params': {'statistic': 'max', 'window': 4}},\n",
    "     {'kind': 'rolling', 'params': {'statistic': 'max', 'window': 8}},\n",
    "     {'kind': 'rolling', 'params': {'statistic': 'max', 'window': 12}},\n",
    "     {'kind': 'rolling', 'params': {'statistic': 'max', 'window': 26}},\n",
    "     {'kind': 'rolling', 'params': {'statistic': 'max', 'window': 52}},\n",
    "     {'kind': 'rolling', 'params': {'statistic': 'max', 'window': 104}}],\n",
    "    'groupby': None,\n",
    "    'source': 'sales'},\n",
    "   {'feature_eng_list': [{'kind': 'lag', 'params': {'lag': 0}},\n",
    "     {'kind': 'lag', 'params': {'lag': 1}},\n",
    "     {'kind': 'lag', 'params': {'lag': 2}},\n",
    "     {'kind': 'lag', 'params': {'lag': 3}},\n",
    "     {'kind': 'rolling', 'params': {'statistic': 'mean', 'window': 4}},\n",
    "     {'kind': 'rolling', 'params': {'statistic': 'mean', 'window': 8}},\n",
    "     {'kind': 'rolling', 'params': {'statistic': 'mean', 'window': 12}},\n",
    "     {'kind': 'rolling', 'params': {'statistic': 'mean', 'window': 26}},\n",
    "     {'kind': 'rolling', 'params': {'statistic': 'mean', 'window': 52}},\n",
    "     {'kind': 'rolling', 'params': {'statistic': 'mean', 'window': 104}},\n",
    "     {'kind': 'rolling', 'params': {'statistic': 'std', 'window': 4}},\n",
    "     {'kind': 'rolling', 'params': {'statistic': 'std', 'window': 8}},\n",
    "     {'kind': 'rolling', 'params': {'statistic': 'std', 'window': 12}},\n",
    "     {'kind': 'rolling', 'params': {'statistic': 'std', 'window': 26}},\n",
    "     {'kind': 'rolling', 'params': {'statistic': 'std', 'window': 52}},\n",
    "     {'kind': 'rolling', 'params': {'statistic': 'std', 'window': 104}}],\n",
    "    'groupby': ['Warehouse'],\n",
    "    'source': 'sales'},\n",
    "   {'feature_eng_list': [{'kind': 'lag', 'params': {'lag': 0}},\n",
    "     {'kind': 'lag', 'params': {'lag': 1}},\n",
    "     {'kind': 'lag', 'params': {'lag': 2}},\n",
    "     {'kind': 'lag', 'params': {'lag': 3}},\n",
    "     {'kind': 'lag', 'params': {'lag': 4}},\n",
    "     {'kind': 'lag', 'params': {'lag': 5}},\n",
    "     {'kind': 'lag', 'params': {'lag': 6}},\n",
    "     {'kind': 'lag', 'params': {'lag': 7}},\n",
    "     {'kind': 'lag', 'params': {'lag': 11}},\n",
    "     {'kind': 'lag', 'params': {'lag': 15}},\n",
    "     {'kind': 'lag', 'params': {'lag': 19}},\n",
    "     {'kind': 'lag', 'params': {'lag': 25}},\n",
    "     {'kind': 'lag', 'params': {'lag': 51}},\n",
    "     {'kind': 'lag', 'params': {'lag': 103}}],\n",
    "    'groupby': ['Client'],\n",
    "    'source': 'sales'},\n",
    "   {'feature_eng_list': [{'kind': 'lag', 'params': {'lag': 0}},\n",
    "     {'kind': 'lag', 'params': {'lag': 1}},\n",
    "     {'kind': 'lag', 'params': {'lag': 2}},\n",
    "     {'kind': 'lag', 'params': {'lag': 3}}],\n",
    "    'groupby': ['Product'],\n",
    "    'source': 'sales'}]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4af6c3-9f0a-4ca0-978c-600947653522",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(**config_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75efdb8-e2c2-4237-b1b1-89f4f8c85800",
   "metadata": {},
   "source": [
    "When running it as a script, the following command took about 6 hours (12 minutes per model) on a recent MacBook.\n",
    "\n",
    "Note that executing the notebook would create the mlflow runs and the submission outputs inside the notebook folders, and not at the project root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f7d968-5a40-44f5-8479-73bf9bd97272",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_phase_2_ensemble(config=config, n_estimators = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aecba66-553e-4ad9-8cb9-f27bcc4c543b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
